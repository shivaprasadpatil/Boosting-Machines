{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOOSTING MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patilsh\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "data_file=r'diabetic_data.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dd=pd.read_csv(data_file)\n",
    "dd['readmitted']=np.where(np.logical_or(dd['readmitted']==\"NO\", \n",
    "                                        dd['readmitted']==\">30\"),\n",
    "                          \"NO\",\"YES\")\n",
    "race_dummies=pd.get_dummies(dd['race'])\n",
    "race_dummies.columns=[\"race_\"+x for x in race_dummies.columns]\n",
    "\n",
    "dd=pd.concat([dd,race_dummies],1)\n",
    "drop_vars=[\"race_?\",\"race\"]\n",
    "\n",
    "dd['gender_F']=np.where(dd['gender']==\"Female\",1,0)\n",
    "drop_vars=drop_vars+['gender']\n",
    "\n",
    "dd['age1'],dd['age2']=zip(*dd['age'].apply(lambda x: x.split('-', 1)))\n",
    "\n",
    "\n",
    "dd['age1']=[x.replace('[',\"\") for x in dd['age1']]\n",
    "dd['age2']=[x.replace(')',\"\") for x in dd['age2']]\n",
    "\n",
    "dd['age']=0.5*(pd.to_numeric(dd['age1'])+pd.to_numeric(dd['age2']))\n",
    "\n",
    "drop_vars=drop_vars+['age1','age2']\n",
    "\n",
    "drop_vars+=['weight']\n",
    "\n",
    "dd['admission_type_id'].value_counts()\n",
    "ad_type_dummies=pd.get_dummies(dd['admission_type_id'])\n",
    "ad_type_dummies.columns=[\"ad_type_\"+str(x) for x in \n",
    "                         ad_type_dummies.columns]\n",
    "dd=pd.concat([dd,ad_type_dummies],1)\n",
    "drop_vars+=['ad_type_4']+['admission_type_id']\n",
    "\n",
    "dd['discharge_disposition_id'].value_counts()\n",
    "dis_dispo_dummies=pd.get_dummies(dd['discharge_disposition_id'])\n",
    "dis_dispo_dummies.columns=['dis_dispo_'+str(x) for x \n",
    "                           in dis_dispo_dummies.columns]\n",
    "dd=pd.concat([dd,dis_dispo_dummies],1)\n",
    "drop_vars+=['dis_dispo_'+str(x) for x in \n",
    "            [15,24,9,17,16,19,10,27,12,20]]+['discharge_disposition_id']\n",
    "\n",
    "\n",
    "dd['admission_source_id'].value_counts()\n",
    "ad_source_dummies=pd.get_dummies(dd['admission_source_id'])\n",
    "ad_source_dummies.columns=['ad_source_'+str(x) for \n",
    "                           x in ad_source_dummies.columns]\n",
    "dd=pd.concat([dd,ad_source_dummies],1)\n",
    "drop_vars+=['ad_source_'+str(x) for x in \n",
    "            [8,22,10,11,14,25,13]]+['admission_source_id']\n",
    "\n",
    "dd['payer_code'].value_counts()\n",
    "payer_code_dummies=pd.get_dummies(dd['payer_code'])\n",
    "payer_code_dummies.columns=['payer_code_'+str(x) for \n",
    "                            x in payer_code_dummies.columns]\n",
    "dd=pd.concat([dd,payer_code_dummies],1)\n",
    "drop_vars+=['payer_code_'+str(x) for x in \n",
    "            ['OT','MP','SI','FR']]+['payer_code']\n",
    "\n",
    "dd['medical_specialty'].value_counts()\n",
    "for ms in ['?','InternalMedicine','Emergency/Trauma',\n",
    "           'Family/GeneralPractice','Cardiology','Surgery-General',\n",
    "          'Nephrology','Orthopedics','Orthopedics-Reconstructive',\n",
    "           'Radiologist']:\n",
    "    v_name=ms.replace(\"?\",\"NoRecord\").replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "    dd['ms_'+v_name]=np.where(dd['medical_specialty']==ms,1,0)\n",
    "\n",
    "    drop_vars+=['medical_specialty']+['diag_1','diag_2','diag_3',\n",
    "                                  'examide','citoglipton']\n",
    "\n",
    "\n",
    "dd=dd.drop(drop_vars,1)\n",
    "drop_vars=[]\n",
    "\n",
    "for col in dd.columns:\n",
    "    if(dd[col].dtype==\"object\"):\n",
    "        temp=pd.get_dummies(dd[col],drop_first=True)\n",
    "        temp.columns=[col+'_'+str(x) for x in temp.columns]\n",
    "        drop_vars+=[col]\n",
    "        dd=pd.concat([dd,temp],1)\n",
    "#         print(col)\n",
    "dd=dd.drop(drop_vars,1)\n",
    "\n",
    "\n",
    "dd.columns=[x.replace(\"-\",\"_\").replace(\"?\",\"_\") for x in dd.columns]\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "dd_train, dd_test = train_test_split(dd, test_size = 0.2,random_state=2)\n",
    "y_train=dd_train['readmitted_YES']\n",
    "x_train=dd_train.drop(['readmitted_YES','encounter_id','patient_nbr'],1)\n",
    "y_test=dd_test['readmitted_YES']\n",
    "x_test=dd_test.drop(['readmitted_YES','encounter_id','patient_nbr'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,\n",
    "                    class_weight='balanced',random_state=2),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1000,\n",
    "                        learning_rate=0.5,\n",
    "                        random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should realise here that there are two sets of hyperparameter which you can tune here. One is for the weak learner which in this case is a simple tree stump, another is for the booster. Although this being a weak learner too much tuning might not make much difference as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=2,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=1000, random_state=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy is :', 0.6654711604598604)\n",
      "('Sensitvity is :', 0.5071003206596427)\n",
      "('Specificity is :', 0.6844972758791481)\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(list(zip(y_test,bdt.predict(x_test))),\n",
    "                columns=['real','predicted'])\n",
    "TP=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==1) ])\n",
    "FP=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==1) ])\n",
    "TN=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==0) ])\n",
    "FN=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==0) ])\n",
    "P=TP+FN\n",
    "N=TN+FP\n",
    "\n",
    "#Accuracy\n",
    "Acc=(TP+TN)/(P+N)\n",
    "\n",
    "#Sensitivity\n",
    "Sn=TP/P\n",
    "\n",
    "#Specificity\n",
    "Sp=TN/N\n",
    "\n",
    "print('Accuracy is :',Acc )\n",
    "print('Sensitvity is :',Sn )\n",
    "print('Specificity is :',Sp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patilsh\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "\n",
    "clf=XGBClassifier(objective=\"binary:logistic\",silent=False,\n",
    "                  scale_pos_weight=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |      Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of boosted trees to fit.\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each split, in each level.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  \n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None)\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      " |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      " |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      " |      passed to the `fit` function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |      clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |      clf.fit(X_train, y_train,\n",
      " |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |              eval_metric='logloss',\n",
      " |              verbose=True)\n",
      " |      \n",
      " |      evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable evals_result will contain:\n",
      " |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True)\n",
      " |      Fit gradient boosting classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          Weight for each instance\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int, optional\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals.  If there's more than one,\n",
      " |          will use the last. Returns the model from the last iteration\n",
      " |          (not the best one). If early stopping occurs, the model will\n",
      " |          have three additional fields: bst.best_score, bst.best_iteration\n",
      " |          and bst.best_ntree_limit.\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=0)\n",
      " |  \n",
      " |  predict_proba(self, data, output_margin=False, ntree_limit=0)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameter.s\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.795 (std: 0.002)\n",
      "Parameters: {'gamma': 0.1, 'colsample_bytree': 0.8, 'subsample': 0.6, 'n_estimators': 1000, 'reg_alpha': 0.1, 'max_depth': 6, 'learning_rate': 0.3, 'min_child_weight': 6}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.694 (std: 0.003)\n",
      "Parameters: {'gamma': 0.1, 'colsample_bytree': 0.9, 'subsample': 0.6, 'n_estimators': 500, 'reg_alpha': 0.1, 'max_depth': 6, 'learning_rate': 0.1, 'min_child_weight': 5}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.689 (std: 0.004)\n",
      "Parameters: {'gamma': 0.3, 'colsample_bytree': 0.8, 'subsample': 0.9, 'n_estimators': 500, 'reg_alpha': 0.1, 'max_depth': 4, 'learning_rate': 0.5, 'min_child_weight': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1),\n",
    "                        reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "              \"max_depth\": [2,3,4,5,6],\n",
    "              \"learning_rate\":[0.01,0.05,0.1,0.3,0.5],\n",
    "    \"min_child_weight\":[4,5,6],\n",
    "              \"subsample\":[i/10.0 for i in range(6,10)],\n",
    " \"colsample_bytree\":[i/10.0 for i in range(6,10)],\n",
    "               \"reg_alpha\":[1e-5, 1e-2, 0.1, 1, 100],\n",
    "              \"gamma\":[i/10.0 for i in range(0,5)],\n",
    "    \"n_estimators\":[100,500,700,1000]\n",
    "              }\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(x_train, y_train)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This ran for over two hours on my machine , i think. Why i am not sure because i left it running and went to sleep. Anyways, point is when you are tuning for hyperparameters , be ready for not getting quick results. Its part of the process.\n",
    "\n",
    "Lets now select the top performer here and build our model with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=XGBClassifier(objective=\"binary:logistic\",silent=False,gamma=0.1,\n",
    "                  colsample_bytree=0.8,\n",
    "                  subsample=0.6,n_estimators=1000,\n",
    "                  reg_alpha=0.1,max_depth=6,learning_rate=0.3,\n",
    "                 min_child_weight=6,scale_pos_weight=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0.1, learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=6, missing=None, n_estimators=1000, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0.1, reg_lambda=1,\n",
       "       scale_pos_weight=9, seed=0, silent=False, subsample=0.6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.7689397661393338\n",
      "Sensitvity is : 0.30737517178195145\n",
      "Specificity is : 0.8243905123548512\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(list(zip(y_test,clf.predict(x_test))),\n",
    "                columns=['real','predicted'])\n",
    "TP=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==1) ])\n",
    "FP=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==1) ])\n",
    "TN=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==0) ])\n",
    "FN=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==0) ])\n",
    "P=TP+FN\n",
    "N=TN+FP\n",
    "\n",
    "#Accuracy\n",
    "Acc=(TP+TN)/(P+N)\n",
    "\n",
    "#Sensitivity\n",
    "Sn=TP/P\n",
    "\n",
    "#Specificity\n",
    "Sp=TN/N\n",
    "\n",
    "print('Accuracy is :',Acc )\n",
    "print('Sensitvity is :',Sn )\n",
    "print('Specificity is :',Sp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a good boost in sensitivity but at the same time , doesnt hamper specificity much. We are calling this a good boost , because other algorithms were performing very poorly on the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
